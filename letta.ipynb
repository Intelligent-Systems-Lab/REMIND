{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe6c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "def transform_time_intervals(intervals):\n",
    "    def parse_relative_time(text):\n",
    "        match = re.match(r\"A (few|couple of) (hours|days|weeks|months|years) after\", text)\n",
    "        if match:\n",
    "            qty = 3 if match.group(1) == \"few\" else 2\n",
    "            unit = match.group(2)\n",
    "            delta = {\n",
    "                \"hours\": timedelta(hours=qty),\n",
    "                \"days\": timedelta(days=qty),\n",
    "                \"weeks\": timedelta(weeks=qty),\n",
    "                \"months\": timedelta(days=qty * 30),  # Approximate month as 30 days\n",
    "                \"years\": timedelta(days=qty * 365)  # Approximate year as 365 days\n",
    "            }\n",
    "            return delta[unit]\n",
    "        return None\n",
    "\n",
    "    def generate_timeline(events):\n",
    "        now = datetime.now() - timedelta(minutes=10)  # 固定當前時間\n",
    "        timeline = [now]\n",
    "        for event in reversed(events[1:]):\n",
    "            delta = parse_relative_time(event)\n",
    "            if delta:\n",
    "                now -= delta\n",
    "            timeline.append(now)\n",
    "\n",
    "        return list(reversed(timeline))\n",
    "    \n",
    "    result = generate_timeline(intervals)\n",
    "    if len([dt.strftime(\"%Y-%m-%d %H:%M\") for dt in result]) > 5:\n",
    "        print(f'error:{intervals}')\n",
    "    return [dt.strftime(\"%Y-%m-%d %H:%M\") for dt in result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d10c7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "question_df['time_changed'] = question_df['time_interval'].apply(transform_time_intervals)\n",
    "question_df = question_df.drop(['check', 'evidence', 'explain', 'answer', 'generate_dialogue', 'relationship', 'time_interval'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5cfc54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_df = question_df[question_df['question_type']=='mix']\n",
    "question_df.reset_index(drop=True, inplace=True)\n",
    "len(question_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4f40b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta_client import CreateBlock, Letta, MessageCreate\n",
    "\n",
    "client = Letta(\n",
    "    base_url=\"http://localhost:8283\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "762886e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 1---\n",
      "---process 2---\n",
      "---process 3---\n",
      "---process 4---\n",
      "---process 5---\n",
      "---process 6---\n",
      "---process 7---\n",
      "---process 8---\n",
      "---process 9---\n",
      "---process 10---\n",
      "---process 11---\n",
      "---process 12---\n",
      "---process 13---\n",
      "---process 14---\n",
      "---process 15---\n",
      "---process 16---\n",
      "---process 17---\n",
      "---process 18---\n",
      "---process 19---\n",
      "---process 20---\n",
      "---process 21---\n",
      "---process 22---\n",
      "---process 23---\n",
      "---process 24---\n",
      "---process 25---\n",
      "---process 26---\n",
      "---process 27---\n",
      "---process 28---\n",
      "---process 29---\n",
      "---process 30---\n",
      "---process 31---\n",
      "---process 32---\n",
      "---process 33---\n",
      "---process 34---\n",
      "---process 35---\n",
      "---process 36---\n",
      "---process 37---\n",
      "---process 38---\n",
      "---process 39---\n",
      "---process 40---\n",
      "---process 41---\n",
      "---process 42---\n",
      "---process 43---\n",
      "---process 44---\n",
      "---process 45---\n",
      "---process 46---\n",
      "---process 47---\n",
      "---process 48---\n",
      "---process 49---\n",
      "---process 50---\n",
      "---process 51---\n",
      "---process 52---\n",
      "---process 53---\n",
      "---process 54---\n",
      "---process 55---\n",
      "---process 56---\n",
      "---process 57---\n",
      "---process 58---\n",
      "---process 59---\n",
      "---process 60---\n",
      "---process 61---\n",
      "---process 62---\n",
      "---process 63---\n",
      "---process 64---\n",
      "---process 65---\n",
      "---process 66---\n",
      "---process 67---\n",
      "---process 68---\n",
      "---process 69---\n",
      "---process 70---\n",
      "---process 71---\n",
      "---process 72---\n",
      "---process 73---\n",
      "---process 74---\n",
      "---process 75---\n",
      "---process 76---\n",
      "---process 77---\n",
      "---process 78---\n",
      "---process 79---\n",
      "---process 80---\n",
      "---process 81---\n",
      "---process 82---\n",
      "---process 83---\n",
      "---process 84---\n",
      "---process 85---\n",
      "---process 86---\n",
      "---process 87---\n",
      "---process 88---\n",
      "---process 89---\n",
      "---process 90---\n",
      "---process 91---\n",
      "---process 92---\n",
      "---process 93---\n",
      "---process 94---\n",
      "---process 95---\n",
      "---process 96---\n",
      "---process 97---\n",
      "---process 98---\n",
      "---process 99---\n",
      "---process 100---\n",
      "---process 101---\n",
      "---process 102---\n",
      "---process 103---\n",
      "---process 104---\n",
      "---process 105---\n",
      "---process 106---\n",
      "---process 107---\n",
      "---process 108---\n",
      "---process 109---\n",
      "---process 110---\n",
      "---process 111---\n",
      "---process 112---\n",
      "---process 113---\n",
      "---process 114---\n",
      "---process 115---\n",
      "---process 116---\n",
      "---process 117---\n",
      "---process 118---\n",
      "---process 119---\n",
      "---process 120---\n",
      "---process 121---\n",
      "---process 122---\n",
      "---process 123---\n",
      "---process 124---\n",
      "---process 125---\n",
      "---process 126---\n",
      "---process 127---\n",
      "---process 128---\n",
      "---process 129---\n",
      "---process 130---\n",
      "---process 131---\n",
      "---process 132---\n",
      "---process 133---\n",
      "---process 134---\n",
      "---process 135---\n",
      "---process 136---\n",
      "---process 137---\n",
      "---process 138---\n",
      "---process 139---\n",
      "---process 140---\n",
      "---process 141---\n",
      "---process 142---\n",
      "---process 143---\n",
      "---process 144---\n",
      "---process 145---\n",
      "---process 146---\n",
      "---process 147---\n",
      "---process 148---\n",
      "---process 149---\n",
      "---process 150---\n",
      "---process 151---\n",
      "---process 152---\n",
      "---process 153---\n",
      "---process 154---\n",
      "---process 155---\n",
      "---process 156---\n",
      "---process 157---\n",
      "---process 158---\n",
      "---process 159---\n",
      "---process 160---\n",
      "---process 161---\n",
      "---process 162---\n",
      "---process 163---\n",
      "---process 164---\n",
      "---process 165---\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "\n",
    "for row in range(len(question_df)):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    question = f\"user:{question_df['question'][row]}\"\n",
    "    \n",
    "    # Create a new agent\n",
    "    agent = client.agents.create(\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        embedding=\"openai/text-embedding-3-small\",\n",
    "    )\n",
    "    \n",
    "    # MemGPT process\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            for number, col in enumerate(['first_session_dialogue', 'second_session_dialogue', 'third_session_dialogue', 'fourth_session_dialogue', 'fifth_session_dialogue']):\n",
    "                chatlogs = []\n",
    "                current_time = datetime.strptime(question_df['time_changed'][row][number], \"%Y-%m-%d %H:%M\")\n",
    "                for i in range(math.ceil(len(question_df[col][row])/2)):\n",
    "                    try:\n",
    "                        chatlogs.append({\n",
    "                            \"user\":question_df[col][row][i*2],\n",
    "                            \"assistant\":question_df[col][row][i*2+1],\n",
    "                            \"time\":current_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                        })\n",
    "                    except:\n",
    "                        chatlogs.append({\n",
    "                            \"user\":question_df[col][row][i*2],\n",
    "                            \"assistant\":\"\",\n",
    "                            \"time\":current_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                        })\n",
    "                    current_time = current_time + timedelta(minutes=1)\n",
    "                    \n",
    "                response = client.agents.messages.create(\n",
    "                    agent_id=agent.id,\n",
    "                    messages=[\n",
    "                        MessageCreate(\n",
    "                            role=\"user\",\n",
    "                            content=str(chatlogs),\n",
    "                        ),\n",
    "                    ],\n",
    "                )\n",
    "            success = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}, Retrying...\")\n",
    "            continue\n",
    "        \n",
    "    # get memory\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            response = client.agents.messages.create(\n",
    "                agent_id=agent.id,\n",
    "                messages=[\n",
    "                    MessageCreate(\n",
    "                        role=\"user\",\n",
    "                        content=question,\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "            question_df.loc[row, 'memgpt_response'] = str(response.messages[-1].content)\n",
    "            success = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}, Retrying...\")\n",
    "            continue\n",
    "    \n",
    "    # delete agent\n",
    "    client.agents.delete(agent_id=agent.id)\n",
    "    \n",
    "    if (row+1)%50==0:\n",
    "        question_df.to_json(\"eval_result_memgpt_mix.json\", orient=\"records\", lines=True)\n",
    "question_df.to_json(\"eval_result_memgpt_mix.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06167067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def llm_create(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def llm_response_handler(response:str):\n",
    "    \"\"\"handle llm response format, especially for llama family\"\"\"\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except:\n",
    "        response = response.strip()\n",
    "        try:\n",
    "            return json.loads(re.search(r\"```json(.*?)```\", response, re.DOTALL).group(1).strip())\n",
    "        except:\n",
    "            try:\n",
    "                return json.loads(re.search(r\"```(.*?)```\", response, re.DOTALL).group(1).strip())\n",
    "            except:\n",
    "                return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1261adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt = \"\"\"Your task is to label an answer to a question as ‘CORRECT’ or ‘WRONG’.\n",
    "\n",
    "You will be given the following data:\n",
    "\n",
    "A question (posed by user to assistant).\n",
    "A ‘gold’ (ground truth) answer.\n",
    "A generated answer, which you will score as CORRECT or WRONG.\n",
    "The point of the question is to ask about something assistant should know about the user based on their prior conversations. The gold answer will usually be a concise and short answer that includes the referenced topic.\n",
    "\n",
    "Example:\n",
    "Question: Do you remember what I got the last time I went to Hawaii?\n",
    "Gold answer: A shell necklace\n",
    "\n",
    "The generated answer might be much longer, but you should be generous with your grading—as long as it touches on the same topic as the gold answer, it should be counted as CORRECT.\n",
    "\n",
    "Examples of CORRECT answers:\n",
    "- Oh yeah, that was so fun! I got so much stuff there, including that shell necklace.\n",
    "- I got a ton of stuff... that surfboard, the mug, the necklace, those coasters too.\n",
    "- That cute necklace.\n",
    "\n",
    "Examples of WRONG answers:\n",
    "- Oh yeah, that was so fun! I got so much stuff there, including that mug.\n",
    "- I got a ton of stuff... that surfboard, the mug, those coasters too.\n",
    "- I’m sorry, I don’t remember what you’re talking about.\n",
    "\n",
    "Now it’s time for the real question:\n",
    "Question: {question}\n",
    "Gold answer: {answer}\n",
    "Generated answer: {generate_answer}\n",
    "\n",
    "Only answer CORRECT or WRONG, no need to explain\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51291c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "question_df = question_df[question_df['question_type']=='mix']\n",
    "question_df.reset_index(drop=True, inplace=True)\n",
    "memgpt_df = pd.read_json(\"eval_result_memgpt_mix.json\", lines=True)\n",
    "judge_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5746364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 1---\n",
      "---process 2---\n",
      "---process 3---\n",
      "---process 4---\n",
      "---process 5---\n",
      "---process 6---\n",
      "---process 7---\n",
      "---process 8---\n",
      "---process 9---\n",
      "---process 10---\n",
      "---process 11---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 12---\n",
      "---process 13---\n",
      "---process 14---\n",
      "---process 15---\n",
      "---process 16---\n",
      "---process 17---\n",
      "---process 18---\n",
      "---process 19---\n",
      "---process 20---\n",
      "---process 21---\n",
      "---process 22---\n",
      "---process 23---\n",
      "---process 24---\n",
      "---process 25---\n",
      "---process 26---\n",
      "---process 27---\n",
      "---process 28---\n",
      "---process 29---\n",
      "---process 30---\n",
      "---process 31---\n",
      "---process 32---\n",
      "---process 33---\n",
      "---process 34---\n",
      "---process 35---\n",
      "---process 36---\n",
      "---process 37---\n",
      "---process 38---\n",
      "---process 39---\n",
      "---process 40---\n",
      "---process 41---\n",
      "---process 42---\n",
      "---process 43---\n",
      "---process 44---\n",
      "---process 45---\n",
      "---process 46---\n",
      "---process 47---\n",
      "---process 48---\n",
      "---process 49---\n",
      "---process 50---\n",
      "---process 51---\n",
      "---process 52---\n",
      "---process 53---\n",
      "---process 54---\n",
      "---process 55---\n",
      "---process 56---\n",
      "---process 57---\n",
      "---process 58---\n",
      "---process 59---\n",
      "---process 60---\n",
      "---process 61---\n",
      "---process 62---\n",
      "---process 63---\n",
      "---process 64---\n",
      "---process 65---\n",
      "---process 66---\n",
      "---process 67---\n",
      "---process 68---\n",
      "---process 69---\n",
      "---process 70---\n",
      "---process 71---\n",
      "---process 72---\n",
      "---process 73---\n",
      "---process 74---\n",
      "---process 75---\n",
      "---process 76---\n",
      "---process 77---\n",
      "---process 78---\n",
      "---process 79---\n",
      "---process 80---\n",
      "---process 81---\n",
      "---process 82---\n",
      "---process 83---\n",
      "---process 84---\n",
      "---process 85---\n",
      "---process 86---\n",
      "---process 87---\n",
      "---process 88---\n",
      "---process 89---\n",
      "---process 90---\n",
      "---process 91---\n",
      "---process 92---\n",
      "---process 93---\n",
      "---process 94---\n",
      "---process 95---\n",
      "---process 96---\n",
      "---process 97---\n",
      "---process 98---\n",
      "---process 99---\n",
      "---process 100---\n",
      "---process 101---\n",
      "---process 102---\n",
      "---process 103---\n",
      "---process 104---\n",
      "---process 105---\n",
      "---process 106---\n",
      "---process 107---\n",
      "---process 108---\n",
      "---process 109---\n",
      "---process 110---\n",
      "---process 111---\n",
      "---process 112---\n",
      "---process 113---\n",
      "---process 114---\n",
      "---process 115---\n",
      "---process 116---\n",
      "---process 117---\n",
      "---process 118---\n",
      "---process 119---\n",
      "---process 120---\n",
      "---process 121---\n",
      "---process 122---\n",
      "---process 123---\n",
      "---process 124---\n",
      "---process 125---\n",
      "---process 126---\n",
      "---process 127---\n",
      "---process 128---\n",
      "---process 129---\n",
      "---process 130---\n",
      "---process 131---\n",
      "---process 132---\n",
      "---process 133---\n",
      "---process 134---\n",
      "---process 135---\n",
      "---process 136---\n",
      "---process 137---\n",
      "---process 138---\n",
      "---process 139---\n",
      "---process 140---\n",
      "---process 141---\n",
      "---process 142---\n",
      "---process 143---\n",
      "---process 144---\n",
      "---process 145---\n",
      "---process 146---\n",
      "---process 147---\n",
      "---process 148---\n",
      "---process 149---\n",
      "---process 150---\n",
      "---process 151---\n",
      "---process 152---\n",
      "---process 153---\n",
      "---process 154---\n",
      "---process 155---\n",
      "---process 156---\n",
      "---process 157---\n",
      "---process 158---\n",
      "---process 159---\n",
      "---process 160---\n",
      "---process 161---\n",
      "---process 162---\n",
      "---process 163---\n",
      "---process 164---\n",
      "---process 165---\n"
     ]
    }
   ],
   "source": [
    "for row in range(len(question_df)):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    question = f\"{question_df['question'][row]}\"\n",
    "    answer = f\"{question_df['answer'][row]}\"\n",
    "    judge_df.loc[row, 'memgpt_answer'] = memgpt_df.loc[row, 'memgpt_response']\n",
    "    judge_df.loc[row, 'memgpt_judge'] = llm_create(judge_prompt.format(question=question, answer=answer, generate_answer=judge_df.loc[row, 'memgpt_answer']))\n",
    "    \n",
    "    if (row+1)%20==0:\n",
    "        judge_df.to_json(\"judge_memgpt_mix.json\", orient=\"records\", lines=True)\n",
    "judge_df.to_json(\"judge_memgpt_mix.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memgpt score:0.9096385542168675\n"
     ]
    }
   ],
   "source": [
    "# long\n",
    "print(f\"memgpt score:{len(judge_df[judge_df['memgpt_judge']=='CORRECT'])/len(judge_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5cbabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memgpt short score:0.9467455621301775\n"
     ]
    }
   ],
   "source": [
    "print(f\"memgpt short score:{len(judge_df[judge_df['memgpt_judge']=='CORRECT'])/len(judge_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a78a56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memgpt mix score:0.7636363636363637\n"
     ]
    }
   ],
   "source": [
    "print(f\"memgpt mix score:{len(judge_df[judge_df['memgpt_judge']=='CORRECT'])/len(judge_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
