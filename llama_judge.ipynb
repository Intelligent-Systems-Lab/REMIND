{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judge by llama3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"You will act as a reviewer to score the following returned memory based on their relevance, conciseness and readability.\n",
    "The fewer and more relevant the key messages are, the higher the score will be.\n",
    "Here is the question and answer:{question}\n",
    "\n",
    "The score range is from 1 to 10 of integer, with 1 being the worst and 10 being the best, ranked in order of importance:\n",
    "\n",
    "1. Relevance of the returned information and its ability to support generating similar responses.\n",
    "2. Concisenessâ€” the more concise and noise-free the returned content, the better.\n",
    "3. Readability for humans, maintaining chronological order to preserve memory continuity.\n",
    "\n",
    "Please refer to the following three examples for guidance:\n",
    "A system:{short_memory}, score:{short_score}\n",
    "B system:{base_dialog}, score:{base_score}\n",
    "C system:{base_paragraph}, score:{base_p_score}\n",
    "\n",
    "Next, please score the returned memory for the following systems:\n",
    "\n",
    "D system:{long_memory}\n",
    "\n",
    "E system:{long_recall_memory}\n",
    "\n",
    "Please provide the scores in the following format:\n",
    "```json\n",
    "{{\n",
    "    \"D\":\"1~10\",\n",
    "    \"E\":\"1~10\"\n",
    "}}\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "def llm_create(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": \"llama3.3:70b-instruct-q4_K_M\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\":False\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()['response']\n",
    "\n",
    "def llm_response_handler(response:str):\n",
    "    \"\"\"handle llm response format, especially for llama family\"\"\"\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except:\n",
    "        response = response.strip()\n",
    "        try:\n",
    "            return json.loads(re.search(r\"```json(.*?)```\", response, re.DOTALL).group(1).strip())\n",
    "        except:\n",
    "            try:\n",
    "                return json.loads(re.search(r\"```(.*?)```\", response, re.DOTALL).group(1).strip())\n",
    "            except:\n",
    "                return response\n",
    "            \n",
    "def get_res(question, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s):\n",
    "    res = llm_create(evaluation_prompt.format(question=question, short_memory=short_searched, \n",
    "                                              long_memory=long_searched, long_recall_memory=long_recall_searched, base_dialog=base_dialog_searched, \n",
    "                                              base_paragraph=base_paragraph_searched, short_score=short_s, base_score=base_s, base_p_score=base_p_s))\n",
    "    res_dict = llm_response_handler(res)\n",
    "    # print(res_dict)\n",
    "    # print(evaluation_prompt.format(question=question, short_memory=short_searched, long_memory=long_searched, long_recall_memory=long_recall_searched, base_dialog=base_dialog_searched, base_paragraph=base_paragraph_searched))\n",
    "    return res_dict.get('D'), res_dict.get('E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gpt_df = pd.read_json(\"eval_result_gpt-4o-mini.json\", lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 425---\n"
     ]
    }
   ],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = gpt_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = gpt_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    gpt_df.loc[row, 'long_score_by_llama'] = long_score\n",
    "    gpt_df.loc[row, 'long_recall_score_by_llama'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.662\n",
      "8.586\n"
     ]
    }
   ],
   "source": [
    "number=500\n",
    "observe_df = gpt_df[:number]\n",
    "print(observe_df['long_score_by_llama'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_llama'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "qwen_df = pd.read_json('eval_result_qwen.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 114---\n",
      "---process 336---\n"
     ]
    }
   ],
   "source": [
    "for row in [113, 335]:\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = qwen_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = qwen_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    qwen_df.loc[row, 'long_score_by_llama'] = long_score\n",
    "    qwen_df.loc[row, 'long_recall_score_by_llama'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.594\n",
      "8.516\n"
     ]
    }
   ],
   "source": [
    "number=500\n",
    "observe_df = qwen_df[:number]\n",
    "print(observe_df['long_score_by_llama'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_llama'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gemma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gemma_df = pd.read_json('eval_result_gemma.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 204---\n",
      "---process 205---\n",
      "---process 206---\n",
      "---process 207---\n",
      "---process 208---\n",
      "---process 209---\n",
      "---process 210---\n",
      "---process 211---\n",
      "---process 212---\n",
      "---process 213---\n",
      "---process 214---\n",
      "---process 215---\n",
      "---process 216---\n",
      "---process 217---\n",
      "---process 218---\n",
      "---process 219---\n",
      "---process 220---\n",
      "---process 221---\n",
      "---process 222---\n",
      "---process 223---\n",
      "---process 224---\n",
      "---process 225---\n",
      "---process 226---\n",
      "---process 227---\n",
      "---process 228---\n",
      "---process 229---\n",
      "---process 230---\n",
      "---process 231---\n",
      "---process 232---\n",
      "---process 233---\n",
      "---process 234---\n",
      "---process 235---\n",
      "---process 236---\n",
      "---process 237---\n",
      "---process 238---\n",
      "---process 239---\n",
      "---process 240---\n",
      "---process 241---\n",
      "---process 242---\n",
      "---process 243---\n",
      "---process 244---\n",
      "---process 245---\n",
      "---process 246---\n",
      "---process 247---\n",
      "---process 248---\n",
      "---process 249---\n",
      "---process 250---\n",
      "---process 251---\n",
      "---process 252---\n",
      "---process 253---\n",
      "---process 254---\n",
      "---process 255---\n",
      "---process 256---\n",
      "---process 257---\n",
      "---process 258---\n",
      "---process 259---\n",
      "---process 260---\n",
      "---process 261---\n",
      "---process 262---\n",
      "---process 263---\n",
      "---process 264---\n",
      "---process 265---\n",
      "---process 266---\n",
      "---process 267---\n",
      "---process 268---\n",
      "---process 269---\n",
      "---process 270---\n",
      "---process 271---\n",
      "---process 272---\n",
      "---process 273---\n",
      "---process 274---\n",
      "---process 275---\n",
      "---process 276---\n",
      "---process 277---\n",
      "---process 278---\n",
      "---process 279---\n",
      "---process 280---\n",
      "---process 281---\n",
      "---process 282---\n",
      "---process 283---\n",
      "---process 284---\n",
      "---process 285---\n",
      "---process 286---\n",
      "---process 287---\n",
      "---process 288---\n",
      "---process 289---\n",
      "---process 290---\n",
      "---process 291---\n",
      "---process 292---\n",
      "---process 293---\n",
      "---process 294---\n",
      "---process 295---\n",
      "---process 296---\n",
      "---process 297---\n",
      "---process 298---\n",
      "---process 299---\n",
      "---process 300---\n",
      "---process 301---\n",
      "---process 302---\n",
      "---process 303---\n",
      "---process 304---\n",
      "---process 305---\n",
      "---process 306---\n",
      "---process 307---\n",
      "---process 308---\n",
      "---process 309---\n",
      "---process 310---\n",
      "---process 311---\n",
      "---process 312---\n",
      "---process 313---\n",
      "---process 314---\n",
      "---process 315---\n",
      "---process 316---\n",
      "---process 317---\n",
      "---process 318---\n",
      "---process 319---\n",
      "---process 320---\n",
      "---process 321---\n",
      "---process 322---\n",
      "---process 323---\n",
      "---process 324---\n",
      "---process 325---\n",
      "---process 326---\n",
      "---process 327---\n",
      "---process 328---\n",
      "---process 329---\n",
      "---process 330---\n",
      "---process 331---\n",
      "---process 332---\n",
      "---process 333---\n",
      "---process 334---\n",
      "---process 335---\n",
      "---process 336---\n",
      "---process 337---\n",
      "---process 338---\n",
      "---process 339---\n",
      "---process 340---\n",
      "---process 341---\n",
      "---process 342---\n",
      "---process 343---\n",
      "---process 344---\n",
      "---process 345---\n",
      "---process 346---\n",
      "---process 347---\n",
      "---process 348---\n",
      "---process 349---\n",
      "---process 350---\n",
      "---process 351---\n",
      "---process 352---\n",
      "---process 353---\n",
      "---process 354---\n",
      "---process 355---\n",
      "---process 356---\n",
      "---process 357---\n",
      "---process 358---\n",
      "---process 359---\n",
      "---process 360---\n",
      "---process 361---\n",
      "---process 362---\n",
      "---process 363---\n",
      "---process 364---\n",
      "---process 365---\n",
      "---process 366---\n",
      "---process 367---\n",
      "---process 368---\n",
      "---process 369---\n",
      "---process 370---\n",
      "---process 371---\n",
      "---process 372---\n",
      "---process 373---\n",
      "---process 374---\n",
      "---process 375---\n",
      "---process 376---\n",
      "---process 377---\n",
      "---process 378---\n",
      "---process 379---\n",
      "---process 380---\n",
      "---process 381---\n",
      "---process 382---\n",
      "---process 383---\n",
      "---process 384---\n",
      "---process 385---\n",
      "---process 386---\n",
      "---process 387---\n",
      "---process 388---\n",
      "---process 389---\n",
      "---process 390---\n",
      "---process 391---\n",
      "---process 392---\n",
      "---process 393---\n",
      "---process 394---\n",
      "---process 395---\n",
      "---process 396---\n",
      "---process 397---\n",
      "---process 398---\n",
      "---process 399---\n",
      "---process 400---\n",
      "---process 401---\n",
      "---process 402---\n",
      "---process 403---\n",
      "---process 404---\n",
      "---process 405---\n",
      "---process 406---\n",
      "---process 407---\n",
      "---process 408---\n",
      "---process 409---\n",
      "---process 410---\n",
      "---process 411---\n",
      "---process 412---\n",
      "---process 413---\n",
      "---process 414---\n",
      "---process 415---\n",
      "---process 416---\n",
      "---process 417---\n",
      "---process 418---\n",
      "---process 419---\n",
      "---process 420---\n",
      "---process 421---\n",
      "---process 422---\n",
      "---process 423---\n",
      "---process 424---\n",
      "---process 425---\n",
      "---process 426---\n",
      "---process 427---\n",
      "---process 428---\n",
      "---process 429---\n",
      "---process 430---\n",
      "---process 431---\n",
      "---process 432---\n",
      "---process 433---\n",
      "---process 434---\n",
      "---process 435---\n",
      "---process 436---\n",
      "---process 437---\n",
      "---process 438---\n",
      "---process 439---\n",
      "---process 440---\n",
      "---process 441---\n",
      "---process 442---\n",
      "---process 443---\n",
      "---process 444---\n",
      "---process 445---\n",
      "---process 446---\n",
      "---process 447---\n",
      "---process 448---\n",
      "---process 449---\n",
      "---process 450---\n",
      "---process 451---\n",
      "---process 452---\n",
      "---process 453---\n",
      "---process 454---\n",
      "---process 455---\n",
      "---process 456---\n",
      "---process 457---\n",
      "---process 458---\n",
      "---process 459---\n",
      "---process 460---\n",
      "---process 461---\n",
      "---process 462---\n",
      "---process 463---\n",
      "---process 464---\n",
      "---process 465---\n",
      "---process 466---\n",
      "---process 467---\n",
      "---process 468---\n",
      "---process 469---\n",
      "---process 470---\n",
      "---process 471---\n",
      "---process 472---\n",
      "---process 473---\n",
      "---process 474---\n",
      "---process 475---\n",
      "---process 476---\n",
      "---process 477---\n",
      "---process 478---\n",
      "---process 479---\n",
      "---process 480---\n",
      "---process 481---\n",
      "---process 482---\n",
      "---process 483---\n",
      "---process 484---\n",
      "---process 485---\n",
      "---process 486---\n",
      "---process 487---\n",
      "---process 488---\n",
      "---process 489---\n",
      "---process 490---\n",
      "---process 491---\n",
      "---process 492---\n",
      "---process 493---\n",
      "---process 494---\n",
      "---process 495---\n",
      "---process 496---\n",
      "---process 497---\n",
      "---process 498---\n",
      "---process 499---\n",
      "---process 500---\n"
     ]
    }
   ],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = gemma_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = gemma_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    gemma_df.loc[row, 'long_score_by_llama'] = long_score\n",
    "    gemma_df.loc[row, 'long_recall_score_by_llama'] = long_recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.528\n",
      "7.448\n"
     ]
    }
   ],
   "source": [
    "number=500\n",
    "observe_df = gemma_df[:number]\n",
    "print(observe_df['long_score_by_llama'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_llama'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "llama_3_3_df = pd.read_json('eval_result_llama3_3.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 171---\n"
     ]
    }
   ],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = llama_3_3_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = llama_3_3_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    llama_3_3_df.loc[row, 'long_score_by_llama'] = long_score\n",
    "    llama_3_3_df.loc[row, 'long_recall_score_by_llama'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.424\n",
      "8.304\n"
     ]
    }
   ],
   "source": [
    "number=500\n",
    "observe_df = llama_3_3_df[:number]\n",
    "print(observe_df['long_score_by_llama'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_llama'].astype(int).sum()/number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
