{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judge by qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"You will act as a reviewer to score the following returned memory based on their relevance, conciseness and readability.\n",
    "The fewer and more relevant the key messages are, the higher the score will be.\n",
    "Here is the question and answer:{question}\n",
    "\n",
    "The score range is from 1 to 10, with 1 being the worst and 10 being the best, ranked in order of importance:\n",
    "\n",
    "1. Relevance of the returned information and its ability to support generating similar responses.\n",
    "2. Concisenessâ€” the more concise and noise-free the returned content, the better.\n",
    "3. Readability for humans, maintaining chronological order to preserve memory continuity.\n",
    "\n",
    "Please refer to the following three examples for guidance:\n",
    "A system:{short_memory}, score:{short_score}\n",
    "B system:{base_dialog}, score:{base_score}\n",
    "C system:{base_paragraph}, score:{base_p_score}\n",
    "\n",
    "Next, please score the returned memory for the following systems:\n",
    "\n",
    "D system:{long_memory}\n",
    "\n",
    "E system:{long_recall_memory}\n",
    "\n",
    "Please provide the scores in the following format:\n",
    "```json\n",
    "{{\n",
    "    \"D\":\"1~10\",\n",
    "    \"E\":\"1~10\"\n",
    "}}\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "def llm_create(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": \"qwen2.5:32b-instruct-q4_K_M\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\":False\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()['response']\n",
    "\n",
    "def llm_response_handler(response:str):\n",
    "    \"\"\"handle llm response format, especially for llama family\"\"\"\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except:\n",
    "        response = response.strip()\n",
    "        try:\n",
    "            return json.loads(re.search(r\"```json(.*?)```\", response, re.DOTALL).group(1).strip())\n",
    "        except:\n",
    "            try:\n",
    "                return json.loads(re.search(r\"```(.*?)```\", response, re.DOTALL).group(1).strip())\n",
    "            except:\n",
    "                return response\n",
    "            \n",
    "def get_res(question, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s):\n",
    "    res = llm_create(evaluation_prompt.format(question=question, short_memory=short_searched, \n",
    "                                              long_memory=long_searched, long_recall_memory=long_recall_searched, base_dialog=base_dialog_searched, \n",
    "                                              base_paragraph=base_paragraph_searched, short_score=short_s, base_score=base_s, base_p_score=base_p_s))\n",
    "    res_dict = llm_response_handler(res)\n",
    "    # print(res_dict)\n",
    "    # print(evaluation_prompt.format(question=question, short_memory=short_searched, long_memory=long_searched, long_recall_memory=long_recall_searched, base_dialog=base_dialog_searched, base_paragraph=base_paragraph_searched))\n",
    "    return res_dict.get('D'), res_dict.get('E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gpt_df = pd.read_json(\"eval_result_gpt-4o-mini.json\", lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = gpt_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = gpt_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    gpt_df.loc[row, 'long_score_by_qwen'] = long_score\n",
    "    gpt_df.loc[row, 'long_recall_score_by_qwen'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number=500\n",
    "observe_df = gpt_df[:number]\n",
    "print(observe_df['long_score_by_qwen'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_qwen'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "qwen_df = pd.read_json('eval_result_qwen.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = qwen_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = qwen_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    qwen_df.loc[row, 'long_score_by_qwen'] = long_score\n",
    "    qwen_df.loc[row, 'long_recall_score_by_qwen'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number=500\n",
    "observe_df = qwen_df[:number]\n",
    "print(observe_df['long_score_by_qwen'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_qwen'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gemma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gemma_df = pd.read_json('eval_result_gemma.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = gemma_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = gemma_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    gemma_df.loc[row, 'long_score_by_qwen'] = long_score\n",
    "    gemma_df.loc[row, 'long_recall_score_by_qwen'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number=500\n",
    "observe_df = gemma_df[:number]\n",
    "print(observe_df['long_score_by_qwen'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_qwen'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "llama_3_3_df = pd.read_json('eval_result_llama3_3.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = llama_3_3_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = llama_3_3_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    llama_3_3_df.loc[row, 'long_score_by_qwen'] = long_score\n",
    "    llama_3_3_df.loc[row, 'long_recall_score_by_qwen'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number=500\n",
    "observe_df = llama_3_3_df[:number]\n",
    "print(observe_df['long_score_by_qwen'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_qwen'].astype(int).sum()/number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
