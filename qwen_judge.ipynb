{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judge by qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"You will act as a reviewer to score the following returned memory based on their relevance, conciseness and readability.\n",
    "The fewer and more relevant the key messages are, the higher the score will be.\n",
    "Here is the question and answer:{question}\n",
    "\n",
    "The score range is from 1 to 10, with 1 being the worst and 10 being the best, ranked in order of importance:\n",
    "\n",
    "1. Relevance of the returned information and its ability to support generating similar responses.\n",
    "2. Concisenessâ€” the more concise and noise-free the returned content, the better.\n",
    "3. Readability for humans, maintaining chronological order to preserve memory continuity.\n",
    "\n",
    "Please refer to the following three examples for guidance:\n",
    "A system:{short_memory}, score:{short_score}\n",
    "B system:{base_dialog}, score:{base_score}\n",
    "C system:{base_paragraph}, score:{base_p_score}\n",
    "\n",
    "Next, please score the returned memory for the following systems:\n",
    "\n",
    "D system:{long_memory}\n",
    "\n",
    "E system:{long_recall_memory}\n",
    "\n",
    "Please provide the scores in the following format:\n",
    "```json\n",
    "{{\n",
    "    \"D\":\"1~10\",\n",
    "    \"E\":\"1~10\"\n",
    "}}\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "def llm_create(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": \"qwen2.5:32b-instruct-q4_K_M\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\":False\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()['response']\n",
    "\n",
    "def llm_response_handler(response:str):\n",
    "    \"\"\"handle llm response format, especially for llama family\"\"\"\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except:\n",
    "        response = response.strip()\n",
    "        try:\n",
    "            return json.loads(re.search(r\"```json(.*?)```\", response, re.DOTALL).group(1).strip())\n",
    "        except:\n",
    "            try:\n",
    "                return json.loads(re.search(r\"```(.*?)```\", response, re.DOTALL).group(1).strip())\n",
    "            except:\n",
    "                return response\n",
    "            \n",
    "def get_res(question, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s):\n",
    "    res = llm_create(evaluation_prompt.format(question=question, short_memory=short_searched, \n",
    "                                              long_memory=long_searched, long_recall_memory=long_recall_searched, base_dialog=base_dialog_searched, \n",
    "                                              base_paragraph=base_paragraph_searched, short_score=short_s, base_score=base_s, base_p_score=base_p_s))\n",
    "    res_dict = llm_response_handler(res)\n",
    "    # print(res_dict)\n",
    "    # print(evaluation_prompt.format(question=question, short_memory=short_searched, long_memory=long_searched, long_recall_memory=long_recall_searched, base_dialog=base_dialog_searched, base_paragraph=base_paragraph_searched))\n",
    "    return res_dict.get('D'), res_dict.get('E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gpt_df = pd.read_json(\"eval_result_gpt-4o-mini.json\", lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 77---\n"
     ]
    }
   ],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = gpt_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = gpt_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    gpt_df.loc[row, 'long_score_by_qwen'] = long_score\n",
    "    gpt_df.loc[row, 'long_recall_score_by_qwen'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.214\n",
      "7.562\n"
     ]
    }
   ],
   "source": [
    "number=500\n",
    "observe_df = gpt_df[:number]\n",
    "print(observe_df['long_score_by_qwen'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_qwen'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "qwen_df = pd.read_json('eval_result_qwen.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 35---\n",
      "---process 184---\n",
      "---process 238---\n"
     ]
    }
   ],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = qwen_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = qwen_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    qwen_df.loc[row, 'long_score_by_qwen'] = long_score\n",
    "    qwen_df.loc[row, 'long_recall_score_by_qwen'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.092\n",
      "7.75\n"
     ]
    }
   ],
   "source": [
    "number=500\n",
    "observe_df = qwen_df[:number]\n",
    "print(observe_df['long_score_by_qwen'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_qwen'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gemma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gemma_df = pd.read_json('eval_result_gemma.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 308---\n",
      "---process 309---\n",
      "---process 310---\n",
      "---process 311---\n",
      "---process 312---\n",
      "---process 313---\n",
      "---process 314---\n",
      "---process 315---\n",
      "---process 316---\n",
      "---process 317---\n",
      "---process 318---\n",
      "---process 319---\n",
      "---process 320---\n",
      "---process 321---\n",
      "---process 322---\n",
      "---process 323---\n",
      "---process 324---\n",
      "---process 325---\n",
      "---process 326---\n",
      "---process 327---\n",
      "---process 328---\n",
      "---process 329---\n",
      "---process 330---\n",
      "---process 331---\n",
      "---process 332---\n",
      "---process 333---\n",
      "---process 334---\n",
      "---process 335---\n",
      "---process 336---\n",
      "---process 337---\n",
      "---process 338---\n",
      "---process 339---\n",
      "---process 340---\n",
      "---process 341---\n",
      "---process 342---\n",
      "---process 343---\n",
      "---process 344---\n",
      "---process 345---\n",
      "---process 346---\n",
      "---process 347---\n",
      "---process 348---\n",
      "---process 349---\n",
      "---process 350---\n",
      "---process 351---\n",
      "---process 352---\n",
      "---process 353---\n",
      "---process 354---\n",
      "---process 355---\n",
      "---process 356---\n",
      "---process 357---\n",
      "---process 358---\n",
      "---process 359---\n",
      "---process 360---\n",
      "---process 361---\n",
      "---process 362---\n",
      "---process 363---\n",
      "---process 364---\n",
      "---process 365---\n",
      "---process 366---\n",
      "---process 367---\n",
      "---process 368---\n",
      "---process 369---\n",
      "---process 370---\n",
      "---process 371---\n",
      "---process 372---\n",
      "---process 373---\n",
      "---process 374---\n",
      "---process 375---\n",
      "---process 376---\n",
      "---process 377---\n",
      "---process 378---\n",
      "---process 379---\n",
      "---process 380---\n",
      "---process 381---\n",
      "---process 382---\n",
      "---process 383---\n",
      "---process 384---\n",
      "---process 385---\n",
      "---process 386---\n",
      "---process 387---\n",
      "---process 388---\n",
      "---process 389---\n",
      "---process 390---\n",
      "---process 391---\n",
      "---process 392---\n",
      "---process 393---\n",
      "---process 394---\n",
      "---process 395---\n",
      "---process 396---\n",
      "---process 397---\n",
      "---process 398---\n",
      "---process 399---\n",
      "---process 400---\n",
      "---process 401---\n",
      "---process 402---\n",
      "---process 403---\n",
      "---process 404---\n",
      "---process 405---\n",
      "---process 406---\n",
      "---process 407---\n",
      "---process 408---\n",
      "---process 409---\n",
      "---process 410---\n",
      "---process 411---\n",
      "---process 412---\n",
      "---process 413---\n",
      "---process 414---\n",
      "---process 415---\n",
      "---process 416---\n",
      "---process 417---\n",
      "---process 418---\n",
      "---process 419---\n",
      "---process 420---\n",
      "---process 421---\n",
      "---process 422---\n",
      "---process 423---\n",
      "---process 424---\n",
      "---process 425---\n",
      "---process 426---\n",
      "---process 427---\n",
      "---process 428---\n",
      "---process 429---\n",
      "---process 430---\n",
      "---process 431---\n",
      "---process 432---\n",
      "---process 433---\n",
      "---process 434---\n",
      "---process 435---\n",
      "---process 436---\n",
      "---process 437---\n",
      "---process 438---\n",
      "---process 439---\n",
      "---process 440---\n",
      "---process 441---\n",
      "---process 442---\n",
      "---process 443---\n",
      "---process 444---\n",
      "---process 445---\n",
      "---process 446---\n",
      "---process 447---\n",
      "---process 448---\n",
      "---process 449---\n",
      "---process 450---\n",
      "---process 451---\n",
      "---process 452---\n",
      "---process 453---\n",
      "---process 454---\n",
      "---process 455---\n",
      "---process 456---\n",
      "---process 457---\n",
      "---process 458---\n",
      "---process 459---\n",
      "---process 460---\n",
      "---process 461---\n",
      "---process 462---\n",
      "---process 463---\n",
      "---process 464---\n",
      "---process 465---\n",
      "---process 466---\n",
      "---process 467---\n",
      "---process 468---\n",
      "---process 469---\n",
      "---process 470---\n",
      "---process 471---\n",
      "---process 472---\n",
      "---process 473---\n",
      "---process 474---\n",
      "---process 475---\n",
      "---process 476---\n",
      "---process 477---\n",
      "---process 478---\n",
      "---process 479---\n",
      "---process 480---\n",
      "---process 481---\n",
      "---process 482---\n",
      "---process 483---\n",
      "---process 484---\n",
      "---process 485---\n",
      "---process 486---\n",
      "---process 487---\n",
      "---process 488---\n",
      "---process 489---\n",
      "---process 490---\n",
      "---process 491---\n",
      "---process 492---\n",
      "---process 493---\n",
      "---process 494---\n",
      "---process 495---\n",
      "---process 496---\n",
      "---process 497---\n",
      "---process 498---\n",
      "---process 499---\n",
      "---process 500---\n"
     ]
    }
   ],
   "source": [
    "for row in range(307, 500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = gemma_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = gemma_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    gemma_df.loc[row, 'long_score_by_qwen'] = long_score\n",
    "    gemma_df.loc[row, 'long_recall_score_by_qwen'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.888\n",
      "6.324\n"
     ]
    }
   ],
   "source": [
    "number=500\n",
    "observe_df = gemma_df[:number]\n",
    "print(observe_df['long_score_by_qwen'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_qwen'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "llama_3_3_df = pd.read_json('eval_result_llama3_3.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 75---\n"
     ]
    }
   ],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = llama_3_3_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = llama_3_3_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    llama_3_3_df.loc[row, 'long_score_by_qwen'] = long_score\n",
    "    llama_3_3_df.loc[row, 'long_recall_score_by_qwen'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.576\n",
      "7.382\n"
     ]
    }
   ],
   "source": [
    "number=500\n",
    "observe_df = llama_3_3_df[:number]\n",
    "print(observe_df['long_score_by_qwen'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_qwen'].astype(int).sum()/number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deepseek-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "deepseek_df = pd.read_json('eval_result_deepseek_v3.json', lines=True)\n",
    "question_df = pd.read_json('questions_0205.json', lines=True)\n",
    "base_df = pd.read_json(\"eval_result_base.json\", lines=True)\n",
    "base_score = pd.read_json('eval_result_base_score.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---process 70---\n",
      "---process 319---\n"
     ]
    }
   ],
   "source": [
    "for row in range(500):\n",
    "    print(f'---process {row+1}---')\n",
    "    \n",
    "    qa_pair = f\"user:{question_df['question'][row]}, assistant:{question_df['answer'][row]}\"\n",
    "    \n",
    "    long_searched = deepseek_df.loc[row, 'long_mem_result']\n",
    "    long_recall_searched = deepseek_df.loc[row, 'long_mem_recall_result']\n",
    "    \n",
    "    short_searched = base_df.loc[row, 'short_mem_result']\n",
    "    base_dialog_searched = base_df.loc[row, 'base_dialog']\n",
    "    base_paragraph_searched = base_df.loc[row, 'base_paragraph']\n",
    "    \n",
    "    short_s = base_score.loc[row, 'short_score']\n",
    "    base_s = base_score.loc[row, 'base_dialog_score']\n",
    "    base_p_s = base_score.loc[row, 'base_paragraph_score']\n",
    "    \n",
    "    long_score, long_recall_score= get_res(qa_pair, short_searched, long_searched, long_recall_searched, base_dialog_searched, base_paragraph_searched, short_s, base_s, base_p_s)\n",
    "    deepseek_df.loc[row, 'long_score_by_llama'] = long_score\n",
    "    deepseek_df.loc[row, 'long_recall_score_by_llama'] = long_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.128\n",
      "8.066\n"
     ]
    }
   ],
   "source": [
    "number=500\n",
    "observe_df = deepseek_df[:number]\n",
    "print(observe_df['long_score_by_llama'].astype(int).sum()/number)\n",
    "print(observe_df['long_recall_score_by_llama'].astype(int).sum()/number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
